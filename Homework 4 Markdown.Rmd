---
title: "Homework 4 Markdown"
author: "Samuel Guzman"
date: "10/5/2020"
output: html_document
---
1. Write a function that takes a vector and returns one bootstrapped sample from said vector. Demonstrate that it works.

```{r first_chunk}

# The function takes a vector (vec) and a sample size (samp_size)
# Then it generates a bootstrapped sample and returns it
get_bootstrapped_sample_from_vec <- function(vec, samp_size){
  bootstrapped_sample <- sample(vec, samp_size, replace = TRUE) #resamples from vec
  return(bootstrapped_sample)
}

# Demonstration that it works: 
vec <- c(5,10,15,20,25,30,35,40)
samp_size <- 4
get_bootstrapped_sample_from_vec(vec, samp_size) 

# It successfully works.
```

2. Write a function that given a vector of values a request for some number of bootstraps (let’s call the parameter R), and a sample statistic function (e.g., mean, IQR, etc.) returns R number of values of that statistic. Have it default to R = 1000 and the function is mean. Show this works for 10 bootstrapped replicate draws of a mean from some vector. Do the values look reasonable? Compare to the actual mean of the vector.

```{r second_chunk}

# INTERPRETATION OF THE QUESTION
# The information that the function is given includes:
# + a "vector of values"
# + a "request for some number of bootstraps (R)"
# + a "sample statistic function (e.g., mean, IQR, etc.)

# The function should return:
# + R number of values of that statistic.

# Key Point:
# "Make sure you are using the function(s) you wrote in #1"

# EXPLANATION OF MY CODE
# There are two new functions that interact 
# (however, there are three in total when including the old one from #1)

# The second function listed below, return_a_stat_based_on_a_vec,
# calculates and returns the statistic for a SINGLE sample.
# The first function, return_r_num_of_values_of_a_stat, calls the second
# function, return_a_stat_based_on_a_vec, R number of times while 
# structuring it in a data frame.

# Therefore, a data frame is produced with the sample statistic in each row
# for R number of rows. This fulfills part of what the question asks. 

# Function for returning R number of values of a statistic
# for R number of bootstraps
return_r_num_of_values_of_a_stat <- function(vec, R, stat_fun){

  stat_R_times <- replicate(R, return_a_stat_based_on_a_vec(vec, stat_fun)) #get stat for a single bootstrap
  stats_in_df <<- data.frame(Values = stat_R_times) # I had to use <<- to get this variable to work in question #3
  return(stats_in_df)
  
}

# Function for getting the specified stat for a single bootstrap 
return_a_stat_based_on_a_vec <- function(vec, stat_fun){
  # Get bootstrapped sample from a vec. This uses the function from #1
  bootstrapped_sample <- get_bootstrapped_sample_from_vec(vec, samp_size) 
  
  # Calculate and return the sample statistic specified by stat_fun.
  if(stat_fun == "mean"){
  bootstrap_sample_mean <- mean(bootstrapped_sample)
  return(bootstrap_sample_mean)
  }
  
  if(stat_fun == "IQR"){
  bootstrap_sample_IQR <- IQR(bootstrapped_sample)
  return(bootstrap_sample_IQR)
  }
  
  if(stat_fun == "median"){
  bootstrap_sample_median <- median(bootstrapped_sample)
  return(bootstrap_sample_median)
  }
}

# DEFAULTS
R <- 1000
stat_fun <- "mean"

# "Show this works for 10 bootstrapped replicate draws of a mean" 
R <- 10
stat_fun <- "mean"
samp_size <- 5

#return_a_stat_based_on_a_vec(vec, R, stat_fun)
return_r_num_of_values_of_a_stat(vec, R, stat_fun)

```

Do the values look reasonable? Compare to the actual mean of the vector.

The original vector is c(5,10,15,20,25,30,35,40). The actual mean of this vector is 22.5. The values look reasonable, because they are generally around the actual mean.

3. Write a function that, given a vector of values, a request for some number of bootstraps, and a sample statistic function, returns the original value of the statistic as applied to the vector, the mean of the statistic generated by the bootstrapped reps, the upper and lower 95% CI of the bootstrapped statistic (e.g., the 0.025 and 0.975 quantile), and the bias (i.e., the original value of the statistic - the mean of the bootstrapped statistic).

```{r third_chunk}

# INTERPRETATION OF THE QUESTION
# The information that the vector is given includes:
# + a "vector of values" (same as before)
# + a "request for some number of bootstraps (R)" (same as before)
# + a "sample statistic function (e.g., mean, IQR, etc.) (same as before)

# The function should return:
# + the "original value of the statistic as applied to the vector"
# + the "mean of the statistic generated by the bootstrapped reps"
# + the "upper and lower 95% CI of the bootstrapped statistic (e.g., the 0.025 and 0.975 quantile)"
# + the "bias (i.e., the original value of the statistic - the mean of the bootstrapped statistic)"

# Key Point:
# "make sure you are using the function(s) you wrote in #1 and/or #2"

get_new_information <- function(vec, R, stat_fun, stats_in_df){
  
  # Determine the "original value of the statistic as applied to the vector"
  if(stat_fun == "mean"){
  stat_original <- mean(vec)
  }
  
  if(stat_fun == "IQR"){
  stat_original <- IQR(vec)
  }
  
  if(stat_fun == "median"){
  stat_original <- median(vec)  
  }
  
  # Determine the "mean of the statistic generated by the bootstrapped reps"
  # Interpretation: it is asking for the mean of all the values in 
  # the data frame made from question #2, which are statistics
  
  mean_of_statistic <- mean(stats_in_df$Values)
  
  # Determine the "upper and lower 95% CI of the bootstrapped statistic
  # (e.g., the 0.025 and 0.975 quantile)"
  
   upper_quantile <- quantile(stats_in_df$Values, probs = 0.025) 
   lower_quantile <- quantile(stats_in_df$Values, probs = 0.975)
   
  # Determine the "bias (i.e., the original value of the statistic -
  # the mean of the bootstrapped statistic)"
  
  the_bias <- (stat_original - mean_of_statistic)
  
  final_df <- data.frame(Original_Value_of_Stat = stat_original, 
                         Mean_of_Statistics_Generated = mean_of_statistic,
                         Upper_Quantile = upper_quantile,
                         Lower_Quantile = lower_quantile,
                         Bias = the_bias)
  
  return(final_df)
  
}

get_new_information(vec, R, stat_fun, stats_in_df)

# Note regarding "make sure you are using the function(s) you 
# wrote in #1 and/or #2":
# I fulfilled this requirement through utilizing variables
# from the earlier functions when they first ran initially,
# specifically, stats_in_df$Values

# Also, I had to use <<- with stats_in_df in Q#2 or it produced errors

```

4. FiveThirtyEight keeps a great archive of poll data at https://projects.fivethirtyeight.com/polls/. The presidential general election polling data is freely available at https://projects.fivethirtyeight.com/polls-page/president_polls.csv with question, poll id, and cycle defining a unique poll.

4a. Download and look at the data. Is it long or wide?

The data is wide. I know this because all column names are listed in the top row, consistent with wide. Furthermore, long data would have taken the column name as a "key" and then in the next column listed the value for that key. This is not what we are seeing in the data.

4b. Get just the polling data for this last week (from 9/29 to today). Filter on start_date. Also filter down to just Biden and Trump (see candidate_name or answer). Extra credit for using {lubridate} for this, but you can just do a messy %in% string match.

```{r fourth_chunk}
# list.files() # To help determine where R is looking for files
# After running, it listed "Data", and the markdown and html files.
# It just needs to look in the Data folder for the csv file.

library(readr)
sheet1 <- read.csv("Data/president_polls.csv")

# When I look at the csv file, there are six rows from 9/29 to today,
# And they are listed right at the top just underneath column names

library(dplyr)
sheet1 %>%
group_by(start_date) #Make sure data is organized by start_date
```

All polling data for this last week.

```{r fifth_chunk}
this_last_week <- sheet1[c(1:6), ]; #rows one through 6 (i.e. this last week).
# I double checked that 1:6 is the correct range of values
this_last_week
```

Filter on start_date for last week and Trump and Biden.

```{r eighth_chunk}

#str(sheet1$start_date)
library(lubridate)
library(dplyr)
library(tidyr)

dates <- c("9/29/2020", "9/30/2020", "10/1/2020", "10/2/2020", "10/3/2020", "10/4/2020", "10/5/2020", "10/6/2020", "10/7/2020")

polling_data <- read.csv("Data/president_polls.csv") #get data from csv file
# Filter the data to start_date for last week and answer to Biden and Trump 
filtered_polling_data <- filter(polling_data, polling_data[,20] == dates & polling_data[,34] == c("Trump","Biden"))

polling_data_processed <- data.frame(State = filtered_polling_data[,4], Start_Date = filtered_polling_data[,20], Answer = filtered_polling_data[,34], Percentage = filtered_polling_data[,38])
polling_data_processed



```

4c. OK, this is your sample. What’s the bootstrapped average percentage for each candidate for nationwide polls (state == "")? Note, this answer will not match 538 given their weighting by poll trustworthiness.

```{r ninth_chunk}
#Interpretation of the question
#Rather than average percentage, a bootstrapped average percentage is the average taken after the values have been bootstrapped.


# The data will need to be filtered down further to get the nationwide polls (i.e. state == "")
filtered_polling_data_2 <- filter(polling_data, polling_data[,20] == dates & 
                                  polling_data[,34] == c("Trump","Biden") &
                                  polling_data[,4] == "")

polling_data_processed_2 <- data.frame(State = filtered_polling_data_2[,4], Start_Date = filtered_polling_data_2[,20], Answer = filtered_polling_data_2[,34], Percentage = filtered_polling_data_2[,38])
polling_data_processed_2

# Create separate vectors for each candidate
Biden_vec <- c(56, 49) #Biden percentages
bootstrap_Biden <- sample(Biden_vec, size = length(Biden_vec)) #one bootstrap for Biden
mean(bootstrap_Biden) #Biden bootstrapped average percentage.

Trump_vec <- c(37) #Trump percentages
bootstrap_Trump <- sample(Trump_vec, size = length(Trump_vec)) #one bootstrap for Trump
mean(bootstrap_Trump) #Trump bootstrapped average percentage.

# As we would expect for Biden, it is within the range of 49-56, so it is reasonable.
# The mean for Trump seems artificially low. I loaded sample(37) in the console and recieved great variation in the numbers. Conceptually they should all be the same, and the mean should be the same (so, 37)

```

4d. What is the average difference between the two candidates by state and national polls? Note, you’ll need to make this a wide data frame to answer! And, well, try the pivot without this advice first, but then….

```{r tenth_chunk}
#First, I am assuming that we are still using the filtered data for just this last week. Normally I interpret a natural progression from one question to the next, and the question did not say otherwise.

#polling_data_processed #This variable is the combined data for last week
#polling_data_processed_2 #This variable is the national poll data for last week
#polling_data_processed_3 #This variable I created below and represents just the state poll data.

# To produce the variable for just the state data.

filtered_polling_data_3 <- filter(polling_data, polling_data[,20] == dates & 
                                  polling_data[,34] == c("Trump","Biden") &
                                  polling_data[,4] != "")

polling_data_processed_3 <- data.frame(State = filtered_polling_data_3[,4], Start_Date = filtered_polling_data_3[,20], Answer = filtered_polling_data_3[,34], Percentage = filtered_polling_data_3[,38])

```

Here are the three data frames that are relevant. First, the combined data:
```{r eleventh_chunk}
polling_data_processed
```

Second, the national poll data:
```{r twelth_chunk}
polling_data_processed_2
```

Third, the state poll data:

```{r thirteenth_chunk}
polling_data_processed_3
```

The issue with all three is that the Trump and Biden values appear in the same columns (in Answer and in pct). It would be easier to calculate the average difference if there was a column for Biden and a column for Trump. This is why we should use pivot.

First, for the national data.

```{r fourteenth_chunk}
polling_data_processed_2_wide <- pivot_wider(polling_data_processed_2,
                                             names_from = Answer,
                                             values_from = Percentage)

polling_data_processed_2_wide
```

Now the differences can be calculated, and later, subsequently, the average of the differences calculated. The only issue though is that Trump has an NA value. The issue likely arose, because it was cut outside of the range for this last week.

```{r fifteenth_chunk}
polling_data_processed_2_wide <- pivot_wider(polling_data_processed_2,
                                             names_from = Answer,
                                             values_from = Percentage)

library(dplyr)
polling_data_processed_2_wide %>%
  na.omit() %>% #ommit NA rows
  mutate(Difference = Biden - Trump) #Calculate the difference

mean(polling_data_processed_2_wide$Difference) #Calculate the average of the differences
#For some reason, this does not work. It returns NA even though the data is clearly there.

# The way that I have been writing the code is that I've created new data frames that no longer contain the excess data. So, there is no need to select and then pivot. I just pivoted that data frame. Creating the new column using paste would require accessing data that is not in the data frame. I could get it from an earlier variable but I don't think it would be lined up properly.

```

Second, for the state data.

```{r sixteenth_chunk}
#polling_data_processed_3_wide <- pivot_wider(polling_data_processed_3,
#                                             names_from = Answer,
#                                            values_from = Percentage)
#
#polling_data_processed_3_wide
```

I recieved this error after loading the prior code block.

## Warning: Values in `Percentage` are not uniquely identified; output will contain list-cols.
## * Use `values_fn = list(Percentage = list)` to suppress this warning.
## * Use `values_fn = list(Percentage = length)` to identify where the duplicates arise
## * Use `values_fn = list(Percentage = summary_fun)` to summarise duplicates

It appears to be because of repeat date values.

```{r seventeenth_chunk}
#polling_data_processed_3_wide <- pivot_wider(polling_data_processed_3,
#                                             names_from = Answer,
#                                             values_from = Percentage)
#
#library(dplyr)
#polling_data_processed_3_wide %>%
#  na.omit() %>% #ommit NA rows
#  mutate(Difference = Biden - Trump) #Calculate the difference


```

5. replicate() has been our friend, but we’ve always had to be a little hacky with it. We’ve either had to fold in means, or use tricksy functions like colMeans and the like.

5a. So, I want you to, using the mean and SD of Biden’s national polling average (you’ll need to calculate it!) from above, simulate 1000 draws from that population with a sample size of 50. What are the dimensions of the object. What are in the rows and columns?

It still isn't clear whether to continue using the subset of the data for this last week or the Biden's national polling for the entire CSV. However, Biden only has two data values for national polling if using the subset (I hope that I did that correctly but it might not be). For this reason, (and also he says sample size 50) I will try using the Biden's national data for the whole CSV.

```{r eighteenth_chunk}

#filtered_polling_data_4 is for all dates, not just this last week
#and it is filtered to national data and Trump and Biden
#filtered_polling_data_4 <- filter(polling_data, polling_data[,34] == c("Trump","Biden") &
#                                                polling_data[,4] == "") #To filter to national data: ""
#
#polling_data_processed_4 <- data.frame(Answer = filtered_polling_data_4[,34], Percentage = #filtered_polling_data_4[,38])
#
#
#polling_data_processed_4_wide <- pivot_wider(polling_data_processed_4,
#                                             names_from = Answer,
#                                             values_from = Percentage)
#
#polling_data_processed_4_wide

```

Once again, it is producing an error. Therefore, I will go back to the initial plan of just using the two data values.

```{r nineteenth_chunk}

BidenVec <- c(56, 49)
mean(BidenVec) #Calculate the mean
sd(BidenVec) #Calculate the sd

# I am going to use the functions that I made in 2 and 3

vec <- BidenVec
R <- 1000 #1000 draws
stat_fun <- "mean"
samp_size <- 2 #2 national data values.

return_r_num_of_values_of_a_stat(vec, R, stat_fun) #Function from 2.

dim(stats_in_df) # Get dimensions.

#----OLD CODE FOR A PREVIOUS ATTEMPT BEFORE MSGING JARETT-----------------

#get_new_information(vec, R, stat_fun, stats_in_df)


#bootstrapped_sample <- sample(BidenVec, 2)
#draws_1000 <- replicate(1000, bootstrapped_sample)
#draws_1000

#dim(draws_1000)

```

The dimensions are 1000 x 1. There are 1000 simulations (rows) and the one column is the mean that was determined from each bootstrap sample.

5b. Yuck. Can you turn this into something usable? Say, first make it a tibble or data frame, and then pivot it to long, such that you end up with a column that has an identifier for sim and a column with a single value from that sim?

```{r twentieth_chunk}

# First, the data is already in data frame format. It is a data frame called stats_in_df. Secondly, there already is a column with a single value from each sim (thus pivoting is unnecessary). I think that the question is asked this way, because people may have written code differently in questions 1-3. Nonetheless, it would be useful to add a column for sim number.

stats_in_df_with_sims <- stats_in_df %>%
   mutate(Sims = seq.int(nrow(stats_in_df)))

stats_in_df_with_sims[ , c(2, 1)] # Reorder columns by index to look neater.

#----OLD CODE FOR A PREVIOUS ATTEMPT BEFORE MSGING JARETT-----------------

#draws_1000_df <- as.data.frame(draws_1000) # Convert to data frame
#draws_1000_df #The data frame

# The final product should be "a column that has an identifier for sim and a column with a single value from that sim". 

#The sim column most likely means the numbers 1 or 2 (representing each of the two sims in the sample). The "column with a single value from that sim" is probably the 1000 draws with all of the values per each sim.

# Before pivoting, there must be a column for sims.
# Create sims column.

#draws_1000_df_with_sims_col <- draws_1000_df %>%
#   mutate(Sims = seq.int(nrow(draws_1000_df)))

# Pivot to long.

#draws_1000_df_long <- pivot_longer(draws_1000_df_with_sims_col,
#                                   cols = starts_with('V'),
#                                   names_to = "Draw",
#                                   values_to = "Value_of_Sim")

# The final product.
#draws_1000_df_long

```

5c. For each sim, what’s the bootsrapped mean and CI? Plot it! And tell us how often it’s greater than the initial mean. E.C. for the plot showing the stats in order from low to high.

```{r twenty-first_chunk}
# To determine the bootstrapped mean and CI, use the function from question 3. Vec is still the Biden vec, R is still 1000, stat_fun is still mean, stats_in_df is up to date. So, there is no need to change the values.

get_new_information(vec, R, stat_fun, stats_in_df)

```

The first time I ran this, the mean of the bootstrapped statistic ended up being 52.6575 and the confidence interval ended up being 49 (upper quantile) - 56 (lower quantile). I am not sure if "bootstrapped mean" is the "mean of the bootstrapped statistic" or the initial statistic (mean) applied to one bootstrap. I am leaning towards the latter. 

```{r twenty-second_chunk}
# Plot of all sims and their values.
library(ggplot2)
pen_plot_base <- ggplot(data = stats_in_df_with_sims,
                        mapping = aes(x = Sims,
                                      y = Values))

pen_plot_base +
  geom_point(size = 3,
             color = "blue")


```

This probably happened, because of the lack of a sufficiently large number of Biden sample points for the national polls for this last week. There are only 2. The initial mean was 52.5. Estimating from the chart, it seems that the greatest portion is on the initial mean exactly, maybe 40%(?). It is unclear from just the image but it still looks 'heavier'/thicker. The rest is divided evenly above and below, so 30% above and 30% below. To answer the question: 30% of the time it is greater than the initial mean.

5d. So…. what is that plot showing? What are the concepts involved?

The plot is showing the mean of each of the 1,000 bootstrapped samples (mean of bootstrapped sample is the y axis). The X axis is the number representing that bootstrap or the number of that draw (e.g. 1 - 1000)